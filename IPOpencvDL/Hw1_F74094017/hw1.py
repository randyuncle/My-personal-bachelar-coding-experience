# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'untitled.ui'
#
# Created by: PyQt5 UI code generator 5.15.10
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.


from PyQt5 import QtCore, QtGui, QtWidgets
from PIL import Image
import torchvision
import torchvision.transforms as transforms
import torch
import torchsummary
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import os
import matplotlib.pyplot as plt
import numpy as np
import cv2

# ---------------------------------------
# The CIFAR10 training section
# ---------------------------------------
classes = ('airplane', 'automobile', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# Define a function to load and preprocess CIFAR-10 data
def load_cifar10(batch_size):
    transform = transforms.Compose(
        [transforms.RandomHorizontalFlip(),
         transforms.RandomCrop(32, padding=4),
         transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

    return trainloader, testloader

# Define the VGG19 model
def vgg19_bn():
    vgg19 = models.vgg19_bn(num_classes=10)
    return vgg19

# Define a function to train the model
def train(model, trainloader, testloader, num_epochs, learning_rate, save_path):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)

    train_loss_history = []
    train_accuracy_history = []
    val_loss_history = []
    val_accuracy_history = []
    best_val_accuracy = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0

        for i, data in enumerate(trainloader, 0):
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        train_accuracy = 100 * correct_train / total_train
        train_loss = running_loss / len(trainloader)
        train_loss_history.append(train_loss)
        train_accuracy_history.append(train_accuracy)

        model.eval()
        val_loss = 0.0
        correct_val = 0
        total_val = 0

        with torch.no_grad():
            for data in testloader:
                inputs, labels = data
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)

                val_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        val_accuracy = 100 * correct_val / total_val
        val_loss = val_loss / len(testloader)
        val_loss_history.append(val_loss)
        val_accuracy_history.append(val_accuracy)

        print(f'Epoch [{epoch + 1}/{num_epochs}] | '
              f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}% | '
              f'Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.2f}%')

        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy
            torch.save(model.state_dict(), save_path)

    print("Training finished.")
    return train_loss_history, train_accuracy_history, val_loss_history, val_accuracy_history

# ---------------------------------------
# The PyQT5 control section
# ---------------------------------------

image_1 = None
image_2 = None

class Ui_MainWindow(object):   
    def __init__(self) -> None:
        self.sobel_x_image = None
        self.sobel_y_image = None
        self.sobel_image = None
        self.test_image = None
        self.vgg19_bn = vgg19_bn() # Build a VGG19 model with batch normalization
    
    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.resize(800, 600)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName("centralwidget")
        self.Load_Image_1 = QtWidgets.QPushButton(self.centralwidget)
        self.Load_Image_1.setGeometry(QtCore.QRect(40, 150, 101, 25))
        self.Load_Image_1.setObjectName("Load_Image_1")
        self.Load_Image_2 = QtWidgets.QPushButton(self.centralwidget)
        self.Load_Image_2.setGeometry(QtCore.QRect(40, 340, 101, 25))
        self.Load_Image_2.setObjectName("Load_Image_2")
        self.Image_Processing = QtWidgets.QGroupBox(self.centralwidget)
        self.Image_Processing.setGeometry(QtCore.QRect(180, 0, 251, 161))
        self.Image_Processing.setObjectName("Image_Processing")
        self.color_seperation = QtWidgets.QPushButton(self.Image_Processing)
        self.color_seperation.setGeometry(QtCore.QRect(30, 40, 191, 25))
        self.color_seperation.setObjectName("color_seperation")
        self.color_transformation = QtWidgets.QPushButton(self.Image_Processing)
        self.color_transformation.setGeometry(QtCore.QRect(30, 80, 191, 25))
        self.color_transformation.setObjectName("color_transformation")
        self.color_extraction = QtWidgets.QPushButton(self.Image_Processing)
        self.color_extraction.setGeometry(QtCore.QRect(30, 120, 191, 25))
        self.color_extraction.setObjectName("color_extraction")
        self.Image_Smoothing = QtWidgets.QGroupBox(self.centralwidget)
        self.Image_Smoothing.setGeometry(QtCore.QRect(180, 170, 251, 161))
        self.Image_Smoothing.setObjectName("Image_Smoothing")
        self.gaussian_blur = QtWidgets.QPushButton(self.Image_Smoothing)
        self.gaussian_blur.setGeometry(QtCore.QRect(30, 40, 191, 25))
        self.gaussian_blur.setObjectName("gaussian_blur")
        self.bilateral_filter = QtWidgets.QPushButton(self.Image_Smoothing)
        self.bilateral_filter.setGeometry(QtCore.QRect(30, 80, 191, 25))
        self.bilateral_filter.setObjectName("bilateral_filter")
        self.median_filter = QtWidgets.QPushButton(self.Image_Smoothing)
        self.median_filter.setGeometry(QtCore.QRect(30, 120, 191, 25))
        self.median_filter.setObjectName("median_filter")
        self.groupBox_3 = QtWidgets.QGroupBox(self.centralwidget)
        self.groupBox_3.setGeometry(QtCore.QRect(180, 340, 251, 201))
        self.groupBox_3.setObjectName("groupBox_3")
        self.sobel_x = QtWidgets.QPushButton(self.groupBox_3)
        self.sobel_x.setGeometry(QtCore.QRect(10, 40, 221, 25))
        self.sobel_x.setObjectName("sobel_x")
        self.sobel_y = QtWidgets.QPushButton(self.groupBox_3)
        self.sobel_y.setGeometry(QtCore.QRect(10, 80, 221, 25))
        self.sobel_y.setObjectName("sobel_y")
        self.combination_and_threshold = QtWidgets.QPushButton(self.groupBox_3)
        self.combination_and_threshold.setGeometry(QtCore.QRect(10, 120, 221, 21))
        self.combination_and_threshold.setObjectName("combination_and_threshold")
        self.gradient_angle = QtWidgets.QPushButton(self.groupBox_3)
        self.gradient_angle.setGeometry(QtCore.QRect(10, 160, 221, 25))
        self.gradient_angle.setObjectName("gradient_angle")
        self.Image_Processing_2 = QtWidgets.QGroupBox(self.centralwidget)
        self.Image_Processing_2.setGeometry(QtCore.QRect(500, 0, 251, 221))
        self.Image_Processing_2.setObjectName("Image_Processing_2")
        self.transform = QtWidgets.QPushButton(self.Image_Processing_2)
        self.transform.setGeometry(QtCore.QRect(30, 180, 191, 25))
        self.transform.setObjectName("transform")
        self.label = QtWidgets.QLabel(self.Image_Processing_2)
        self.label.setGeometry(QtCore.QRect(20, 40, 67, 17))
        self.label.setObjectName("label")
        self.label_2 = QtWidgets.QLabel(self.Image_Processing_2)
        self.label_2.setGeometry(QtCore.QRect(20, 70, 67, 17))
        self.label_2.setObjectName("label_2")
        self.label_3 = QtWidgets.QLabel(self.Image_Processing_2)
        self.label_3.setGeometry(QtCore.QRect(20, 100, 67, 17))
        self.label_3.setObjectName("label_3")
        self.label_4 = QtWidgets.QLabel(self.Image_Processing_2)
        self.label_4.setGeometry(QtCore.QRect(20, 130, 67, 17))
        self.label_4.setObjectName("label_4")
        self.rotation_lineEdit = QtWidgets.QLineEdit(self.Image_Processing_2)
        self.rotation_lineEdit.setGeometry(QtCore.QRect(90, 40, 81, 25))
        self.rotation_lineEdit.setObjectName("rotation_lineEdit")
        self.scailing_lineEdit = QtWidgets.QLineEdit(self.Image_Processing_2)
        self.scailing_lineEdit.setGeometry(QtCore.QRect(90, 70, 81, 25))
        self.scailing_lineEdit.setObjectName("scailing_lineEdit")
        self.tx_lineEdit = QtWidgets.QLineEdit(self.Image_Processing_2)
        self.tx_lineEdit.setGeometry(QtCore.QRect(90, 100, 81, 25))
        self.tx_lineEdit.setObjectName("tx_lineEdit")
        self.ty_lineEdit = QtWidgets.QLineEdit(self.Image_Processing_2)
        self.ty_lineEdit.setGeometry(QtCore.QRect(90, 130, 81, 25))
        self.ty_lineEdit.setObjectName("ty_lineEdit")
        self.label_5 = QtWidgets.QLabel(self.Image_Processing_2)
        self.label_5.setGeometry(QtCore.QRect(190, 40, 41, 17))
        self.label_5.setObjectName("label_5")
        self.label_6 = QtWidgets.QLabel(self.Image_Processing_2)
        self.label_6.setGeometry(QtCore.QRect(190, 100, 41, 17))
        self.label_6.setObjectName("label_6")
        self.label_7 = QtWidgets.QLabel(self.Image_Processing_2)
        self.label_7.setGeometry(QtCore.QRect(190, 130, 41, 17))
        self.label_7.setObjectName("label_7")
        self.groupBox_4 = QtWidgets.QGroupBox(self.centralwidget)
        self.groupBox_4.setGeometry(QtCore.QRect(500, 230, 251, 321))
        self.groupBox_4.setObjectName("groupBox_4")
        self.load_image = QtWidgets.QPushButton(self.groupBox_4)
        self.load_image.setGeometry(QtCore.QRect(60, 30, 131, 25))
        self.load_image.setObjectName("load_image")
        self.show_agumented_images = QtWidgets.QPushButton(self.groupBox_4)
        self.show_agumented_images.setGeometry(QtCore.QRect(10, 70, 221, 21))
        self.show_agumented_images.setObjectName("show_agumented_images")
        self.show_model_structure = QtWidgets.QPushButton(self.groupBox_4)
        self.show_model_structure.setGeometry(QtCore.QRect(10, 100, 221, 21))
        self.show_model_structure.setObjectName("show_model_structure")
        self.show_acc_and_loss = QtWidgets.QPushButton(self.groupBox_4)
        self.show_acc_and_loss.setGeometry(QtCore.QRect(10, 130, 221, 21))
        self.show_acc_and_loss.setObjectName("show_acc_and_loss")
        self.inference = QtWidgets.QPushButton(self.groupBox_4)
        self.inference.setGeometry(QtCore.QRect(10, 160, 221, 21))
        self.inference.setObjectName("inference")
        self.perdict_name = QtWidgets.QLabel(self.groupBox_4)
        self.perdict_name.setGeometry(QtCore.QRect(50, 190, 151, 17))
        self.perdict_name.setObjectName("perdict_name")
        self.perdict_image = QtWidgets.QGraphicsView(self.groupBox_4)
        self.perdict_image.setGeometry(QtCore.QRect(60, 210, 131, 101))
        self.perdict_image.setObjectName("perdict_image")
        self.image1_name = QtWidgets.QLabel(self.centralwidget)
        self.image1_name.setGeometry(QtCore.QRect(40, 190, 101, 17))
        self.image1_name.setObjectName("image1_name")
        self.image2_name = QtWidgets.QLabel(self.centralwidget)
        self.image2_name.setGeometry(QtCore.QRect(40, 380, 101, 17))
        self.image2_name.setObjectName("image2_name")
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 800, 22))
        self.menubar.setObjectName("menubar")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName("statusbar")
        MainWindow.setStatusBar(self.statusbar)
        
        # load image button signal holders
        self.Load_Image_1.clicked.connect(self.open_image1_dialog)
        self.Load_Image_2.clicked.connect(self.open_image2_dialog)
        
        # Section 1 button signal holders
        self.color_seperation.clicked.connect(self.color_seperation_clicked)
        self.color_transformation.clicked.connect(self.color_transformation_clicked)
        self.color_extraction.clicked.connect(self.color_extraction_clicked)
        
        # Section 2 button signal holders
        self.gaussian_blur.clicked.connect(self.gaussian_blur_clicked)
        self.bilateral_filter.clicked.connect(self.bilateral_filter_clicked)
        self.median_filter.clicked.connect(self.median_filter_clicked)
        
        # Section 3 button signal holders
        self.sobel_x.clicked.connect(self.sobel_x_clicked)
        self.sobel_y.clicked.connect(self.sobel_y_clicked)
        self.combination_and_threshold.clicked.connect(self.combination_and_threshold_clicked)
        self.gradient_angle.clicked.connect(self.gradient_angle_clicked)

        # Section 4 button signal holders
        self.transform.clicked.connect(self.transform_clicked)
        
        # Section 5 button signal holders
        self.load_image.clicked.connect(self.load_image_clicked)
        self.show_agumented_images.clicked.connect(self.show_agumented_images_clicked)
        self.show_model_structure.clicked.connect(self.show_model_structure_clicked)
        self.show_acc_and_loss.clicked.connect(self.show_acc_and_loss_clicked)
        self.inference.clicked.connect(self.inference_clicked)

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate("MainWindow", "MainWindow"))
        self.Load_Image_1.setText(_translate("MainWindow", "Load Image 1"))
        self.Load_Image_2.setText(_translate("MainWindow", "Load Image 2"))
        self.Image_Processing.setTitle(_translate("MainWindow", "1. Image Processing"))
        self.color_seperation.setText(_translate("MainWindow", "1.1 Color Seperation"))
        self.color_transformation.setText(_translate("MainWindow", "1.2 Color Transformation"))
        self.color_extraction.setText(_translate("MainWindow", "1.3 Color Extraction"))
        self.Image_Smoothing.setTitle(_translate("MainWindow", "2. Image Smoothing"))
        self.gaussian_blur.setText(_translate("MainWindow", "2.1 Gaussian blur"))
        self.bilateral_filter.setText(_translate("MainWindow", "2.2 Bilateral filter"))
        self.median_filter.setText(_translate("MainWindow", "2.3 Median filter"))
        self.groupBox_3.setTitle(_translate("MainWindow", "3. Edge Detection"))
        self.sobel_x.setText(_translate("MainWindow", "3.1 Sobel X"))
        self.sobel_y.setText(_translate("MainWindow", "3.2 Sobel Y"))
        self.combination_and_threshold.setText(_translate("MainWindow", "3.3 Combination and Threshold"))
        self.gradient_angle.setText(_translate("MainWindow", "3.4 Gradient Angle"))
        self.Image_Processing_2.setTitle(_translate("MainWindow", "4. Transforms"))
        self.transform.setText(_translate("MainWindow", "4. Transform"))
        self.label.setText(_translate("MainWindow", "Rotation:"))
        self.label_2.setText(_translate("MainWindow", "Scailing:"))
        self.label_3.setText(_translate("MainWindow", "Tx:"))
        self.label_4.setText(_translate("MainWindow", "Ty:"))
        self.label_5.setText(_translate("MainWindow", "deg"))
        self.label_6.setText(_translate("MainWindow", "pixel"))
        self.label_7.setText(_translate("MainWindow", "pixel"))
        self.groupBox_4.setTitle(_translate("MainWindow", "5. VGG19"))
        self.load_image.setText(_translate("MainWindow", "Load Image"))
        self.show_agumented_images.setText(_translate("MainWindow", "5.1 Show Agumented Images"))
        self.show_model_structure.setText(_translate("MainWindow", "5.2 Show Model Structure"))
        self.show_acc_and_loss.setText(_translate("MainWindow", "5.3 Show Acc and Loss"))
        self.inference.setText(_translate("MainWindow", "5.4 Inference"))
        self.perdict_name.setText(_translate("MainWindow", "Predict = "))
        self.image1_name.setText(_translate("MainWindow", "No image"))
        self.image2_name.setText(_translate("MainWindow", "No image"))
    
    # holding signal for opening image1
    def open_image1_dialog(self):
        file_name = QtWidgets.QFileDialog.getOpenFileName(self.Load_Image_1, 'open file', '.')
        f_name = file_name[0]
        
        self.image1_name.clear()
        self.image1_name.setText(f_name)
        
        global image_1
        image_1 = cv2.imread(f_name)

    # holding signal for opening image2
    def open_image2_dialog(self):
        file_name = QtWidgets.QFileDialog.getOpenFileName(self.Load_Image_2, 'open file', '.')
        f_name = file_name[0]
        
        self.image2_name.clear()
        self.image2_name.setText(f_name)
        
        global image_2
        image_2 = plt.imread(f_name)
    
    """Section: Image Processing
    1.1 color seperation
    1.2 color transformation
    1.3 color extraction
    """
    def color_seperation_clicked(self):
        global image_1    
        
        b, g, r = cv2.split(image_1)
        
        # Merge the channels to create BGR images
        bgr_blue = cv2.merge((b, b * 0, b * 0))
        bgr_green = cv2.merge((g * 0, g, g * 0))
        bgr_red = cv2.merge((r * 0, r * 0, r))
        
        cv2.imshow('BGR Blue', bgr_blue)
        cv2.imshow('BGR Green', bgr_green)
        cv2.imshow('BGR Red', bgr_red)
        
    def color_transformation_clicked(self):
        global image_1    
            
        # Convert to grayscale
        gray_image_q1 = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)

        # Merge BGR channels and calculate the average
        b, g, r = cv2.split(image_1)
        gray_image_q2 = (b + g + r) / 3

        cv2.imshow('Q1 Grayscale', gray_image_q1)
        cv2.imshow('Q2 Grayscale', gray_image_q2.astype('uint8'))  # Ensure data type is 'uint8'
    
    def color_extraction_clicked(self):
        global image_1
        
        # Transfer the image from BGR to HSV format
        hsv_image = cv2.cvtColor(image_1, cv2.COLOR_BGR2HSV)

        # Define lower and upper bounds for the yellow-green color range in HSV
        lower_bound = np.array([15, 25, 25])
        upper_bound = np.array([85, 255, 255])

        # Create the yellow-green mask (I1)
        yellow_green_mask = cv2.inRange(hsv_image, lower_bound, upper_bound)

        # Convert the yellow-green mask to BGR format
        yellow_green_mask_bgr = cv2.cvtColor(yellow_green_mask, cv2.COLOR_GRAY2BGR)

        # Remove yellow and green colors from the original image (I2)
        removed_yellow_green = cv2.bitwise_not(yellow_green_mask_bgr, image_1, yellow_green_mask)

        cv2.imshow('Yellow-Green Mask (I1)', yellow_green_mask)
        cv2.imshow('Image with Yellow and Green Removed (I2)', removed_yellow_green)
    
    """Section: Image Smoothing
    2.1 gaussian blur
    2.2 bilateral filter
    3.3 median filter
    """
    def gaussian_blur_clicked(self):
        cv2.namedWindow("Gaussian Blur")
        
        radius_min = 1
        radius_max = 5
        
        cv2.createTrackbar("Radius", "Gaussian Blur", radius_min, radius_max, update_gaussian_radius)
        
        # show initial image
        initial_radius = 1
        cv2.setTrackbarPos("Radius", "Gaussian Blur", initial_radius)
        update_gaussian_radius(initial_radius)
    
    def bilateral_filter_clicked(self):
        cv2.namedWindow("Bilateral Filter")
        
        radius_min = 1
        radius_max = 5
        
        cv2.createTrackbar("Radius", "Bilateral Filter", radius_min, radius_max, update_bilateral_radius)
        
        # show initial image
        initial_radius = 1
        cv2.setTrackbarPos("Radius", "Bilateral Filter", initial_radius)
        update_bilateral_radius(initial_radius)
        
    def median_filter_clicked(self):
        cv2.namedWindow("Median Filter")
        
        radius_min = 1
        radius_max = 5
        
        cv2.createTrackbar("Radius", "Median Filter", radius_min, radius_max, update_median_radius)        
        
        # show initial image
        initial_radius = 1
        cv2.setTrackbarPos("Radius", "Median Filter", initial_radius)
        update_median_radius(initial_radius)
        
    """Section: Edge Detection
    3.1 sobel X
    3.2 sobel Y
    3.3 combination and threshold
    3.4 gradient angle 
    """
    def sobel_x_clicked(self):
        global image_1
        
        # convert to gray image
        gray_image = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)
        
        # doing gaussian blur to the gray image
        gray_gussian_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
        x, y = gray_gussian_image.shape
        
        # define sobel x array
        sobel_x = np.array([[-1, 0, 1],
                            [-2, 0, 2],
                            [-1, 0, 1]])
        
        self.sobel_x_image = np.zeros((x, y))
        sobel_x_image = np.zeros((x, y)) # local sobel image (to be normalized and adjusted)
        # sobel x calculation
        for i in range(1, x - 1):
            for j in range(1, y - 1):
                self.sobel_x_image[i, j] = np.sum(sobel_x * gray_gussian_image[i - 1:i + 2, j - 1:j + 2])
                sobel_x_image[i, j] = self.sobel_x_image[i, j]
                # avoid negative number
                if self.sobel_x_image[i, j] < 0:
                    sobel_x_image[i, j] = 0
        
        # normalize the result to avoid noise
        sobel_x_image = cv2.normalize(sobel_x_image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
        
        cv2.imshow("Sobel x Image", sobel_x_image)
        
    def sobel_y_clicked(self):
        global image_1
        
        # convert to gray image
        gray_image = cv2.cvtColor(image_1, cv2.COLOR_BGR2GRAY)
        
        # doing gaussian blur to the gray image
        gray_gussian_image = cv2.GaussianBlur(gray_image, (5, 5), 0)
        x, y = gray_gussian_image.shape
        
        # define sobel y array
        sobel_y = np.array([[-1, -2, -1],
                            [0, 0, 0],
                            [1, 2, 1]])
        
        self.sobel_y_image = np.zeros((x, y))
        sobel_y_image = np.zeros((x, y)) # local sobel image (to be normalized and adjusted)
        # sobel y calculation
        for i in range(1, x - 1):
            for j in range(1, y - 1):
                self.sobel_y_image[i, j] = np.sum(sobel_y * gray_gussian_image[i - 1:i + 2, j - 1:j + 2])
                sobel_y_image[i, j] = self.sobel_y_image[i, j]
                # avoid negative number
                if self.sobel_y_image[i, j] < 0:
                    sobel_y_image[i, j] = 0
        
        # normalize the result to avoid noise
        sobel_y_image = cv2.normalize(sobel_y_image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
        
        cv2.imshow("Sobel y Image", sobel_y_image)
    
    def combination_and_threshold_clicked(self):
        threshold = 128
        
        # G = sprt(gx ** 2 + gy ** 2)
        combined = np.sqrt(self.sobel_x_image ** 2 + self.sobel_y_image ** 2)
        self.sobel_image = cv2.normalize(combined, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)

        # Take `np.where` instead of `cv2.threshold` (some bugs occured while using it)
        thresholded_result = np.where(self.sobel_image >= threshold, 255, 0).astype(np.uint8)
        
        cv2.imshow("Combined Sobel Image", self.sobel_image)
        cv2.imshow("Thresholded Image", thresholded_result)
    
    def gradient_angle_clicked(self):
        # tangent -> θ
        gradient_angle = np.arctan2(self.sobel_y_image, self.sobel_x_image) * 180
        
        # Generate the masks of the selected range of gradient angle
        mask1 = np.where((gradient_angle >= 120) & (gradient_angle <= 180), 255, 0).astype(np.uint8)
        mask2 = np.where((gradient_angle >= 210) & (gradient_angle <= 330), 255, 0).astype(np.uint8)
        
        # Apply `cv2.bitwise_and` to mask the image
        result1 = cv2.bitwise_and(self.sobel_image, self.sobel_image, mask=mask1)
        result2 = cv2.bitwise_and(self.sobel_image, self.sobel_image, mask=mask2)
        
        cv2.imshow("Angle Range 1 (120 to 180)", result1)
        cv2.imshow("Angle Range 2 (210 to 330)", result2)
        
    """Section: Transforms
    4.1 Rotation
    4.2 Scaling
    4.3 Translate
    """
    def transform_clicked(self):
        global image_1
        
        # initialization
        angle = 0 # counter-clockwise
        scale = 1.0
        tx = 0
        ty = 0
        
        # Change the variable if there's something in the line edit text bar
        if self.rotation_lineEdit.text() != '':
            angle = int(self.rotation_lineEdit.text())
        if self.scailing_lineEdit.text() != '':
            scale = float(self.scailing_lineEdit.text())
        if self.tx_lineEdit.text() != '':
            tx = int(self.tx_lineEdit.text())
        if self.ty_lineEdit.text() != '':
            ty = int(self.ty_lineEdit.text())

        h, w = image_1.shape[:2]
        
        center = (w // 2, h // 2)
        
        # Calculate the transformation matrices
        rotation_matrix = cv2.getRotationMatrix2D(center, angle, scale)
        translation_matrix = np.array([[1, 0, tx], [0, 1, ty]], dtype=np.float32)
        # -----------------------------------------------
        # Combine the rotation and transition result to a proper result
        #
        # (There are some problems from doing the transformations independently,
        # which will cause the image (burger.png) to move while rotate 
        # since `cv2.getRotationMatrix2D` rotates the WHOLE image.
        # Therefore, I use the following method to adjust and combine the result.)
        # 
        # np.vstack(): apply the homogeneous coordinate system
        # np.dot(): apply the dot product
        # -----------------------------------------------
        combined_matrix = np.dot(rotation_matrix, np.vstack((translation_matrix, [0, 0, 1])))
        
        # Apply the transformation to the image using `cv2.warpAffine`
        transformed_image = cv2.warpAffine(image_1, combined_matrix, (w, h))
    
        cv2.imshow('Burger', transformed_image) 
        
    """Section: Training a CIFAR10 Classifier Using VGG19 with BN 
    5.1 Load CIFAR10 and show 9 Augmented Images with Labels
    5.2 Load Model and Show Model Structure
    5.3 Show Training/Validating Accuracy and Loss
    5.4 Use the Model with Highest Validation Accuracy to Run Inference, Show the Predicted Distribution and Class Label
    """
    def show_agumented_images_clicked(self):
        folder = 'C:/Users/rkhuncle/Desktop/AI/OpenCvDl_Hw1_Dataset/Q5_image/Q5_1/'
        images = []
        
        data_transforms = transforms.Compose([
            transforms.RandomHorizontalFlip(),
            transforms.RandomVerticalFlip(),
            transforms.RandomRotation(30),
        ])
        
        for filename in os.listdir(folder):
            image_path = os.path.join(folder, filename)
            image = Image.open(image_path)
            augmented_image = data_transforms(image)
            images.append((filename, augmented_image))
        
        # Display the augmented images with labels
        fig, axes = plt.subplots(3, 3, figsize=(8, 10))
        for i, (name, image) in enumerate(images):
            row, col = divmod(i, 3)
            axes[row, col].imshow(image)
            axes[row, col].set_title(name)
            axes[row, col].axis('on')
        
        plt.show()
    
    def show_model_structure_clicked(self):
        # Use torchsummary to show the model structure
        torchsummary.summary(self.vgg19_bn, (3, 224, 224))  # Assuming input size of (3, 224, 224)
    
    def show_acc_and_loss_clicked(self):
        image = cv2.imread('training_history.png')
        cv2.imshow('Training history', image)
    
    def load_image_clicked(self):
        file_name = QtWidgets.QFileDialog.getOpenFileName(self.load_image, 'open file', '.')
        f_name = file_name[0]
        
        # Show the chosen image to the `QGraphicsView`
        scene = QtWidgets.QGraphicsScene()
        scene.setSceneRect(0, 0, 128, 128)
        img = QtGui.QPixmap(f_name)
        img = img.scaled(128,128)
        scene.addPixmap(img)
        self.perdict_image.setScene(scene)
        
        self.test_image = cv2.imread(f_name)
        
    def inference_clicked(self):
        # Load the model
        model = self.vgg19_bn
        model.load_state_dict(torch.load('best_vgg19_bn_cifar10.pth'))
        
        # Transform test data to torch form
        transform = transforms.Compose(
            [transforms.ToTensor(),
             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        
        image = self.test_image
        image_tensor = transform(Image.fromarray(image)).unsqueeze(0)

        # Perdict the image by given model
        with torch.no_grad():
            model.eval()
            outputs = model(image_tensor)
            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
            predicted = torch.argmax(probabilities).item()
            class_label = f'Predicted = {classes[predicted]}'
            self.perdict_name.setText(class_label)
            
            plt.figure()
            plt.bar(classes, probabilities.numpy())
            plt.xlabel('Class')
            plt.xticks(rotation=30)
            plt.ylabel('Probability')
            plt.title('Result of the perdiction of the model')
            plt.show()
            
# function for 'cv2.createTrackbar' for Gaussian Blur to control the task of radius
def update_gaussian_radius(radius):
    global image_1
    
    blurred_image = cv2.GaussianBlur(image_1, (2 * radius + 1, 2 * radius + 1), 0)
    cv2.imshow("Gaussian Blur", blurred_image)

# function for 'cv2.createTrackbar' for Bilateral Filter  to control the task of radius
def update_bilateral_radius(radius):
    global image_1
    
    bilateral_image = cv2.bilateralFilter(image_1, 2 * radius + 1, 90, 90)
    cv2.imshow("Bilateral Filter", bilateral_image)
    
# function for 'cv2.createTrackbar' for Bilateral Filter  to control the task of radius
def update_median_radius(radius):
    global image_1
    
    median_image = cv2.medianBlur(image_1, 2 * radius + 1)
    cv2.imshow("Median Filter", median_image)
                
if __name__ == "__main__":
    """
    # ------------------------------------------
    # Start to train the model
    # ------------------------------------------
    
    # Define hyperparameters
    batch_size = 64
    num_epochs = 40
    learning_rate = 0.01

    # Load CIFAR-10 data
    trainloader, testloader = load_cifar10(batch_size)

    # Initialize VGG19 with Batch Normalization
    model = vgg19_bn()

    # Train the model
    save_path = "best_vgg19_bn_cifar10.pth"
    train_loss_history, train_accuracy_history, val_loss_history, val_accuracy_history = train(model, trainloader, testloader, num_epochs, learning_rate, save_path)

    # Plot training and validation loss and accuracy
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(train_loss_history, label="Training Loss")
    plt.plot(val_loss_history, label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid()

    plt.subplot(1, 2, 2)
    plt.plot(train_accuracy_history, label="Training Accuracy")
    plt.plot(val_accuracy_history, label="Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.grid()

    plt.savefig("training_history.png")
    plt.show()
    """
    
    # ------------------------------------------
    # Trigger PyQT5 scene 
    # ------------------------------------------
    
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())